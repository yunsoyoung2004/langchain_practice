
# LangSmithë¥¼ í™œìš©í•œ ì§ˆë¬¸-ì •ë‹µ AI í‰ê°€ ìë™í™” ìš”ì•½

ì´ ë¬¸ì„œëŠ” LangSmithë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸-ì •ë‹µ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê³ , ìë™ í‰ê°€(Auto-Evaluator)ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ê²€ì¦í•˜ëŠ” ì „ì²´ ì ˆì°¨ë¥¼ ìš”ì•½í•œ ê²ƒì´ë‹¤.

---

## ğŸ“Œ ë°ì´í„°ì…‹ ìƒì„±

```python
inputs = [
  (question[0], correct_answers[0]),
  (question[1], correct_answers[1]),
  (question[2], correct_answers[2])
]

dataset_name = "qanda"
dataset = client.create_dataset(
    dataset_name=dataset_name, 
    description="Questions and Answers"
)

for input_prompt, output_answer in inputs:
    client.create_example(
        inputs={"question": input_prompt},
        outputs={"answer": output_answer},
        dataset_id=dataset.id
    )
```

> âš ï¸ ë™ì¼í•œ ì´ë¦„ì˜ ë°ì´í„°ì…‹ì„ ì¬ìƒì„±í•˜ë ¤ í•˜ë©´ ì¶©ëŒ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ â†’ ì´ë¦„ì„ ë³€ê²½í•˜ê±°ë‚˜ ì‚­ì œ í•„ìš”

---

## ğŸ§ª ì„±ëŠ¥ í‰ê°€ ì¤€ë¹„

1. [https://smith.langchain.com](https://smith.langchain.com) ì ‘ì†
2. `Datasets & Experiments` ë©”ë‰´ í´ë¦­
3. `qanda` ë°ì´í„°ì…‹ ìƒì„± ì—¬ë¶€ í™•ì¸ í›„ í´ë¦­
4. ì˜¤ë¥¸ìª½ ìƒë‹¨ `Add Auto-Evaluator` í´ë¦­
5. Evaluator ìœ í˜•: `LLM-as-a-Judge` ì„ íƒ
6. Evaluator ì„¤ì •:
   - Name: `qanda-evaluator`
   - Provider: `OpenAI`
   - Model: `gpt-4o`
   - Temperature: 0

---

## âš™ï¸ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ì‹¤í–‰

7. Prompt ë©”ë‰´ì—ì„œ `Create a prompt from scratch` í´ë¦­
8. Variable Mapping ì„¤ì • (ì˜ˆ: input â†’ question, output â†’ answer, reference_output â†’ answer)
9. API í‚¤ ì…ë ¥ ë° ì €ì¥
10. `Test over dataset` í´ë¦­
11. ë°ì´í„°ì…‹ìœ¼ë¡œ `qanda` ì„ íƒ
12. `Start` í´ë¦­ í›„ í‰ê°€ ì‹¤í–‰

---

## ğŸ“Š í‰ê°€ ê²°ê³¼ í™•ì¸

- `Datasets & Experiments`ë¡œ ëŒì•„ê°€ì„œ `qanda` ì„ íƒ
- `Correctness` ì§€í‘œ í™•ì¸
  - ê°’ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•í•œ ì‘ë‹µ
  - ì§€í‘œëŠ” ëª¨ë¸ ì‘ë‹µê³¼ ê¸°ëŒ€ ì •ë‹µì˜ ì¼ì¹˜ë„ë¥¼ ë‚˜íƒ€ëƒ„

---

## âœ… ìš”ì•½

LangSmithë¥¼ í†µí•´ ì§ˆë¬¸-ì •ë‹µ ìŒì„ ê¸°ë°˜ìœ¼ë¡œ LLM í‰ê°€ ìë™í™”ê°€ ê°€ëŠ¥í•˜ë©°,
ê°„ë‹¨í•œ ì½”ë“œì™€ ì›¹ UIë¥¼ ì¡°í•©í•´ ì†ì‰½ê²Œ í‰ê°€ ê²°ê³¼ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤.
